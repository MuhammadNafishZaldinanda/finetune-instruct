{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43c3797-2ac7-4e96-a799-130d0878418f",
   "metadata": {},
   "source": [
    "# Convert Model HF to GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c80ac-6472-4fff-8ac8-2cf0a4c0f605",
   "metadata": {},
   "source": [
    "## 1. Git Clone llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e38c54-9cad-424e-99e2-2b20ca98566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp\n",
    "!cd llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a801cb-7dde-4f23-acd9-073c1c1b6150",
   "metadata": {},
   "source": [
    "## 2. Build llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbef6d-d6c8-4226-9824-d05d11fc131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake -B build\n",
    "!cmake --build build --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7b0ab-7eff-48fb-b5e2-5b1d8e43e6f0",
   "metadata": {},
   "source": [
    "Atau jika llama.cpp digunakan hanya untuk convert GGUF perintahnya seperti dibawah ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabbbc7-e1ed-49a5-9ae2-f70e64e4077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake -B build -DLLAMA_CURL=OFF\n",
    "!cmake --build build --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3690fa-97e4-456f-8c19-7322d2c57bfe",
   "metadata": {},
   "source": [
    "Dengan perintah diatas maka hanya bisa digunakan untuk convert model ke GGUF saja, menggunakan -DLLAMA_CURL=OFF saat menjalankan cmake  akan mematikan dukungan untuk fitur yang membutuhkan libcurl, seperti:\n",
    "- llama-download (fitur unduh model dari internet)\n",
    "- Fitur server llama-server (jika menggunakan fitur external KV cache berbasis remote)\n",
    "- Fitur chat CLI berbasis RAG dari URL atau API\n",
    "\n",
    "Namun, **ini tidak akan memengaruhi llama-quantize, karena llama-quantize hanya bekerja pada file .gguf lokal dan tidak bergantung pada libcurl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b046d79-fc63-4f8d-be71-996961f5899e",
   "metadata": {},
   "source": [
    "## 3. Merged Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cc92c-54c7-4af3-9a2e-702c24de1b3d",
   "metadata": {},
   "source": [
    "Tujuan Merge Model\n",
    "1. Membuat model standalone:\n",
    "\n",
    "   - Saat fine-tune model (misal LoRA atau full fine-tuning), perubahan biasanya hanya tersimpan di file delta atau adapter.\n",
    "   - Proses merge menyatukan perubahan itu ke dalam satu model lengkap.\n",
    "\n",
    "\n",
    "2. Distribusi / deployment lebih mudah:\n",
    "\n",
    "   - Hanya satu file .bin atau .gguf yang berisi seluruh bobot model.\n",
    "   - Tidak perlu dependensi tambahan seperti LoRA adapter atau base model.\n",
    "\n",
    "\n",
    "3. Siap untuk kuantisasi dan inference:\n",
    "\n",
    "   - llama.cpp atau llama-quantize butuh bobot model yang sudah digabung, bukan file delta.\n",
    "   - Setelah merge, bisa dilakukan kuantisasi ke Q4_K_M, Q8_0, dsb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a818eeeb-89a3-4ba7-9730-da70745b2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cc632-5d63-40fb-b05e-8307063909c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             cache_dir = \"model_cache/\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"/home/nafis/code/research-llm/NLP/finetuning/FT-qwen3-4b/lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074309e2-2f95-49ad-970f-bf3bb62a1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a233f9a-e6c8-46df-9873-f794dc6cda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "model.save_pretrained(\"merged_model_09092024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a48be-f4b2-4d0d-8db9-8ce6dd0f6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"model_cache/\")\n",
    "tokenizer.save_pretrained(\"merged_model_09092024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bab5f-cc70-4e06-ae6d-5bcb37c52912",
   "metadata": {},
   "source": [
    "## 4. convert_hf_to_gguf.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132bb8b-c85a-4840-9462-4fa9c1954197",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 llama.cpp/convert_hf_to_gguf.py /home/nafis/code/research-llm/NLP/finetuning/merged_model_19052025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7cd57f-1c6d-4a38-898f-99fe1c92571a",
   "metadata": {},
   "source": [
    "## 5. llama-quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6da0ca-9416-4134-bda6-5acc0254f200",
   "metadata": {},
   "source": [
    "sesuaikan dengan quant_type yang ingin digunakan seperti Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8dd01-dbc5-49bb-b372-414822592656",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/nafis/code/research-llm/llama.cpp/build/bin/llama-quantize /home/nafis/code/research-llm/NLP/finetuning/merged_model_19052025/Merged_Model_19052025-4.0B-F16.gguf /home/nafis/code/research-llm/NLP/finetuning/merged_model_19052025/ggml-model-Q4_K_M.gguf Q4_K_M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
